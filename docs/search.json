[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Models",
    "section": "",
    "text": "Preface\n\nRemember that all models are wrong; the practical question is how wrong do they have to be to not be useful.\n—Box & Draper (1987)\n\nTo do:\n\nnetwork formation (Page 2018, 122)\nfriendship paradoxes\nbroadcast, diffusion, contagion\n\n\n\n\n\nPage, Scott E. 2018. The Model Thinker: What You Need to Know to Make Data Work for You. Basic Books."
  },
  {
    "objectID": "intro.html#uses",
    "href": "intro.html#uses",
    "title": "1  Introduction",
    "section": "1.1 Uses",
    "text": "1.1 Uses\n[R] Reason. To identify conditions and deduce logical implications.\n\n…or REDCAPE\n\nWhile it’s true that the conclusions we derive depend on our assumptions, this doesn’t mean all models deliver tautologies. The logical implications of models are sometimes unexpected —e.g. Arrow’s impossibility theorem. Other times they will solve paradoxes —e.g. using causal models to solve Simpson’s paradox. And sometimes they will simply uncover mathematical relationships —e.g. prices and marginal costs.\nAbove all, logic reveals the conditionality of truths; they identify the conditions under which certain claims hold or don’t.\n\nCritics of formalism claim that models repackage what we already know, that they pour old wine into shiny mathematical bottles, that we do not need a model to know that two heads are better than one or that he who hesitates is lost. We can learn the value of commitment from reading of Odysseus tying himself to the mast. That criticism fails to recognize that inferences drawn from models take conditional forms: if \\(A\\) holds, then \\(B\\) follows. Lessons drawn from literature or proverbial advice from great thinkers often provide no conditions. If we try to lead our lives or manage others by unconditional truths, we find ourselves lost in a sea of opposite proverbs.\nPage (2018, 18)\n\nHow can we choose between the following list without knowing which conditions make them true?\n\n\n\n\n\n\n\nProverb\nOpposite\n\n\n\n\nTwo heads are better than one\nToo many cooks spoil the broth\n\n\nHe who hesitates is lost\nA stitch in time saves nine\n\n\nTie yourself to the mast\nKeep your options open\n\n\nThe perfect is the enemy of the good\nDo it well or not at all\n\n\nActions speak louder than words\nThe pen is mightier than the sword\n\n\n\n[E] Explain. To provide (testable) explanations for empirical phenomena.\nNote that models can also explain shape: e.g. network models of influence and contagion often produce S-shape curves.\n\nAs for the claim that models can explain anything: it is true, they can. However, a model-based explanation includes formal assumptions and explicit causal chains. Those assumptions and causal chains can be taken to data. A model that claims that high levels of criminal behavior can be explained by low probabilities of being caught can be tested.\nPage (2018, 19)\n\n[D] Design. To choose features of institutions, policies, and rules.\n[C] Communicate. Models improve communication by creating a common representation, thus enabling the transferring of ideas between different communities of inquiry.\n[A] Act. To guide policy choices and strategic actions.\n[P] Predict. To make numerical and categorical predictions of future and unknown phenomena. Note that prediction differs from explanation.\nFor example, deep-learning algorithms can predict many things with great accuracy, but they offer little in the way of explanation. On the other hand, some models can explain but have little predictive power. This is related to Elster’s (2015) discussion of the fact that we don’t know which “social mechanism” will be triggered in any given situation.\n[E] Explore. To investigate possibilities and hypotheticals —i.e., the “alternative reality approach”."
  },
  {
    "objectID": "intro.html#human-behavior",
    "href": "intro.html#human-behavior",
    "title": "1  Introduction",
    "section": "1.2 Human Behavior",
    "text": "1.2 Human Behavior\nPage (2018) notes that modeling human behavior is challenging because people share six characteristics: we are diverse; we are socially influenced; we are error-prone; we are purposive; we learn; and we have agency.\n\nEach of this six characteristics are potential model features. If we include a feature, we must decide how much of it to include. How diverse do we make our actors? How much social influence do we include? Do people learn from others? How do we define objectives? How much agency do people possess?\nPage (2018, 47)\n\nFor example, to tackle diversity we sometimes assume that behavioral diversity cancels out. And this will only happen if the actions people take are independent (i.e. models of normal distributions).\nThat being said, Page (2018) divides his repertoire of models according to whether we believe humans to follow a logic of consequence (i.e., rational decision-makers) or they follow a logic of appropriateness (i.e., rule-followers).\n\n1.2.1 Rational-choice models\nAn agent is rational if she makes choices towards fulfilling a goal, making the most efficient use of resources.\n\nAn individual’s preferences are represented by a mathematical utility or payoff function defined over a set of possible actions. The individual chooses the action that maximizes the function’s value. In a game, that choice may require beliefs about the actions of other players.\nPage (2018, 48)\n\nIn assuming a utility function, we give preferences a coherency that may not exist. These preferences must satisfy certain axioms in order to be representable by a utility function: completeness, transitivity, independence, and continuity.\nSuppose we have a choice set \\(X = \\{x_1, x_2, \\dots, x_n\\}\\) that contains all available alternatives (e.g. stuff to buy, decisions to make).\n\nCompleteness: all pairs of alternatives can be compared.\nTransitivity: a logical order can be established among them: if \\(x_1 \\succeq x_2\\) and \\(x_2 \\succeq x_3\\), then \\(x_1 \\succeq x_3\\).\nIndependence of irrelevant alternatives.\nContinuity: if we have \\(x_1 \\succeq x_2 \\succeq x_3\\), then there exists a probability \\(p\\) such that \\(x_2 = p\\ x_1 + (1-p) x_3\\).\n\nPeople will violate this axioms under any number of circumstances, leading to a widespread skepticism of rational-actor models. Page responds to these criticisms with four arguments, depicted in Table 1.1.\n\n\nTable 1.1: Arguments for Rational Choice\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\n“As if”\nIntelligent rule-based behavior may be indistinguishable from optimal or near-optimal behavior\n\n\nLearning\nIn situations that are repeated, people should approach optimal behavior\n\n\nLarge stakes\nOn important decisions, people gather information and think slowly\n\n\nUniqueness\nOptimal behavior is often unique, making the model testable/ tractable\n\n\nConsistency\nOptimal behavior creates a consistent model. If people learn the model, they will not change their behavior\n\n\nBenchmark\nOptimal behavior provides a benchmark as an upper bound on people’s cognitive abilities\n\n\n\n\nThe consistency argument is related to “Lucas’ critique”, discussed near the end. Basically, any model that doesn’t predict optimal behavior will fail to make long-term predictions when people have something to gain by optimizing their behavior.\nAlso, numerous studies on “heuristics and cognitive bias” (e.g. loss aversion and hyperbolic discounting) have shown systematic deviations from rational choice.\nThese considerations aside, Page argues that we should always be open to the possibility that rational-actor models will not solve the problem at hand, and that we should privilege other models instead.\n\n\n1.2.2 Rule-based models\nWhereas optimization-based models assume an underlying utility or payoff function that people maximize, rule-based models assume specific behaviors. Many people equate optimization-based models with mathematics and rule-based models with computation, but this distinction is not very clean.\n\nFixed rules. A fixed rule applies the same algorithm (or decision making protocol) at all times. This will provide a lower bound on people’s cognitive abilities. For example, “zero intelligence” is sometimes used as a fixed rule in markets: zero-intelligence traders accept any offer that produces a payoff. Remarkably, encoding this rule in a computer model results in nearly efficient outcomes.\nAdaptive rules. An adaptive rule switches among a set of behaviors, evolves new behaviors, or copies the behaviors of others in order to improve a payoff. Thus, adaptive rules require a utility or payoff function. People like Gerd Gigerenzer argue that people tend toward simple and effective rules within any given situation, and that if that’s what people do, then we should model them this way.\n\nNote that rule-based models make no explicit assumption about rationality, but adaptive-rule models exhibit “ecological rationality”—i.e., better rules eventually predominate.\nFinally, note that some rule-based behaviors approximate rational choice. For example, “buy low, sell high” is a very simple heuristic that will consistently lead to profit in a market economy. Rules like these might be held consciously or unconsciously.\nHow smart should we make the actors in our models?\nIt depends on what type of outcome is produced by the model. We have four options: equilibrium, cycles, randomness, or complexity. If the model produces randomness at an aggregate level, then it’s safe to say that individuals probably can’t learn anything —i.e. they can’t choose optimally. The models that produce cycles or equilibria, on the other hand, create a stationary environment in which we expect people to learn.\nNotice that if adaptive rules produce an equilibrium, then the equilibrium should be consistent with behavior by optimizing agents. Otherwise, optimal behavior will be an unrealistic assumption in complex situations.\nThe Lucas critique\nIf people learn, then we cannot rely on past data to predict outcomes under a policy change. This insight is a variant of Campbell’s law, which states that people respond to any measure in ways that render it less effective. Thus, models must take into account the fact that people respond to policy and environmental changes.\n\n\n\n\nElster, Jon. 2015. Explaining Social Behavior: More Nuts and Bolts for the Social Sciences. Cambridge University Press.\n\n\nPage, Scott E. 2018. The Model Thinker: What You Need to Know to Make Data Work for You. Basic Books."
  },
  {
    "objectID": "sim-00-intro.html",
    "href": "sim-00-intro.html",
    "title": "Simulation",
    "section": "",
    "text": "Some remarks"
  },
  {
    "objectID": "net-00-intro.html#introduction",
    "href": "net-00-intro.html#introduction",
    "title": "Network Models",
    "section": "Introduction",
    "text": "Introduction\n\nAny model we construct, be it of a market, the spread of a disease, or the transmission of information, can be enriched by embedding the actors in a network.\nPage (2018, 117)\n\nA network is a representation of a system that contains discrete, interconnected elements. The elements are represented by nodes (or vertices), and the interconnections are represented by edges (or ties).\nEdges may have attributes like distance or monetary transactions—i.e., weights. They may also be directed or undirected, depending on whether the relationships they represent are asymmetric or symmetric.\nMany real-world problems can be solved using graph algorithms. For example, Dijkstra’s shortest path algorithm is an efficient way to find the shortest path from a node to all other nodes in a graph.\nFormal Presentation\nA graph is a mathematical object that consists of a set of \\(V\\) of vertices (or nodes) and a multiset \\(E\\) of pairs of elements in \\(V\\). It encodes relational information (or connections).\n\nA weighted graph will contain another set of elements associated to each pair of nodes in \\(E\\).\n\n\\[\nG = \\{V, E\\}\n\\]\n\nA multiset is an set in which elements are allowed to appear more than once. For example: \\(\\{a, a, b, c, c, c\\}\\).\n\nWhen two nodes \\(a\\) and \\(b\\) are connected, \\(\\{a, b\\} \\in E\\), we call them adjacent.\nWe can also represent a graph as an adjacency matrix \\(\\mathbf{M}\\) such that each entry \\(m_{ij}\\) is zero when \\(\\{i, j\\} \\not \\in E\\).\n\\[\nm_{ij} = \\begin{cases}\n1 &\\text{if } (v_i, v_j) \\in E \\\\\n0 &\\text{if } (v_i, v_j) \\not \\in E\n\\end{cases}\n\\]\n\n\nCode\n# Adj. Matrix\nset.seed(1111)\nn <- 5\nM <- array(\n  data = sample(1:0, size = n*n, replace = TRUE), \n  dim = c(n, n),\n  dimnames = list(letters[1:n], letters[1:n])\n)\n\ndiag(M) <- 0\n\nknitr::kable(M) |> \n  kableExtra::kable_styling()\n# Graph\n\nlibrary(igraph)\nlibrary(ggraph)\n\nnet <- igraph::graph_from_adjacency_matrix(M, mode = \"directed\") \n\nnet |> \n  ggraph() + \n  geom_node_text(aes(label = name), color = \"steelblue1\", size = 8) + \n  geom_edge_fan(\n    arrow = arrow(length = unit(0.25, \"cm\")),\n    end_cap = circle(.5, \"cm\"),\n    start_cap = circle(.5, \"cm\"),\n    colour = \"steelblue1\"\n  ) +\n  theme_void()\n\n\n\n\n\n\n \n  \n      \n    a \n    b \n    c \n    d \n    e \n  \n \n\n  \n    a \n    0 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    b \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    c \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    d \n    0 \n    0 \n    1 \n    0 \n    1 \n  \n  \n    e \n    0 \n    1 \n    1 \n    0 \n    0 \n  \n\n\n\n\n\n\n\n\n\n\nThe matrix representation is better suited for calculating all sorts of network statistics. In what follows, we will refer to these objects as M and net respectively."
  },
  {
    "objectID": "net-00-intro.html#network-statistics",
    "href": "net-00-intro.html#network-statistics",
    "title": "Network Models",
    "section": "Network Statistics",
    "text": "Network Statistics\n\nNodes\nDegree\nThis measure captures the number of ties for each node. When we look at directed graphs, we need to distinguish between two types of degrees:\n\noutdegree: the number of ties sent from node \\(i\\)\n\nrowSums(M)\n\na b c d e \n4 1 1 2 2 \n\nigraph::degree(net, mode = \"out\")\n\na b c d e \n4 1 1 2 2 \n\n\nindegree: the number of ties received by node \\(i\\)\n\ncolSums(M)\n\na b c d e \n0 2 3 1 4 \n\nigraph::degree(net, mode = \"in\")\n\na b c d e \n0 2 3 1 4 \n\n\n\nThe degree distribution tells us if some nodes are more connected than others. Social networks usually have more equal distributions than networks connecting websites or citations among documents, all of which have long tails.\n\nA power-law network is a network whose degree distribution follows a “power law.”\n\nNeighbors\nThe set of nodes in \\(u\\)’s neighborhood is usually depicted as \\(N_u\\) and \\(|N_u|\\) represents the number of nodes in that neighborhood.\n\nigraph::neighbors(net, v = \"a\")\n\n+ 4/5 vertices, named, from 4ef57b9:\n[1] b c d e\n\nigraph::neighborhood(net)\n\n[[1]]\n+ 5/5 vertices, named, from 4ef57b9:\n[1] a b c d e\n\n[[2]]\n+ 3/5 vertices, named, from 4ef57b9:\n[1] b a e\n\n[[3]]\n+ 4/5 vertices, named, from 4ef57b9:\n[1] c a d e\n\n[[4]]\n+ 4/5 vertices, named, from 4ef57b9:\n[1] d a c e\n\n[[5]]\n+ 5/5 vertices, named, from 4ef57b9:\n[1] e a b c d\n\n\nNote that the igraph::neighborhood() function includes \\(u\\) among \\(N_u\\).\nLocal Clustering Coefficient\nThe clustering coefficient of \\(u\\) is the percentage of \\(u\\)’s pairs of neighbors that are also connected by a tie—e.g., if \\(u\\) has a neighborhood of size 10, then it has \\({10 \\choose 2} = 45\\) pairs of friends; if 15 of those 45 pairs are themselves connected, then \\(u\\)’s clustering coefficient equal \\(\\frac{1}{3}\\).\nI don’t know about the following equations…\n\nFrequency Interpretation:\n\\[\nC_u = \\frac{| \\{{v, w \\in N_u \\mid (v, w) \\in E} \\}|}{|N_u| \\times (|N_u| - 1)}\n\\]\nProbability Interpretation:\n\nBetweenness\nBetweennnes. The number of paths of minimal length connecting two other nodes that pass through one node.\nThe average length between nodes gets shorter as we add more edges to a graph.\n\n\nNetwork\nDensity\nThis measure captures the total number of edges (or ties) in the network, divided by the total number of possible edges.\nGetting to the total number of possible edges requires a little bit of combinatorics:\n\nPossible edges in an undirected network\n\\[\n{n \\choose 2} = \\frac{n!}{(n - 2)! 2!} = \\frac{n (n-1)}{2}\n\\]\nPossible edges in a directed network\n\\[\n\\underbrace{P(n, 2)}_\\text{permutation} = \\frac{n!}{(n-2)!} = n(n-1)\n\\]\nPossible edges in an undirected network (loops allowed)\n\\[\n{n \\choose 2} + \\underbrace{n}_\\text{diagonal} = \\frac{n (n-1)}{2} + n\n\\]\nPossible edges in a directed network (loops allowed)\n\\[\nP(n, 2) + \\underbrace{n}_\\text{diagonal} = n(n-1) + n\n\\]\n\nThus, the density of a graph \\(G = \\{V, E\\}\\) is simply:\n\\[\n\\text{density}(G_n) = \\frac{|E|}{|V| \\cdot (|V| - 1)}\n\\]\nHere is how we perform such a calculation:\n\nn_nodes <- igraph::gorder(net)\nn_edges <- igraph::gsize(net)\nn_possible_edges <- n_nodes * (n_nodes - 1)\n## density\nn_edges / n_possible_edges\n\n[1] 0.5\n\nigraph::edge_density(net)\n\n[1] 0.5\n\n\n\n\nFlow\nWalks\nAny sequence of edges that connect \\(i\\) to \\(j\\). For example, the following sequence is a walk of length 4 from \\(i\\) to \\(j\\):\n\\[\ni \\to k \\to l \\to k \\to j\n\\]\nBy raising the adjacency matrix to the nth power, we get the number of walks of length \\(n\\) between all \\(i,j\\) pairs.\n\nwalks <- function(M, n) {\n  stopifnot(n >= 0, nrow(M) == ncol(M))\n  if (n == 0) {\n   diag(nrow(M))\n  } else{\n   Reduce(`%*%`, rep(list(M), n)) \n  }\n}\n\n## number of walks of length 3\nwalks(M, 3)\n\n  a b c d e\na 0 3 3 0 3\nb 0 0 0 0 2\nc 0 0 0 0 2\nd 0 1 1 0 2\ne 0 2 2 0 0\n\nM %*% M %*% M \n\n  a b c d e\na 0 3 3 0 3\nb 0 0 0 0 2\nc 0 0 0 0 2\nd 0 1 1 0 2\ne 0 2 2 0 0\n\n\nPaths\nPath length. The minimum number of edges that must be traversed to get from one node to another.\nAny sequence of edges that connect \\(i\\) to \\(j\\), where a path is not allowed to revisit the same node twice (unlike walks). We use the igraph::distances() function to get the shortest path (or distance) between every node. The mode = \"out\" argument says we want the distance from \\(i\\) to \\(j\\), which is what we typically want.\n\ndist_mat <- igraph::distances(net, mode = \"out\")\ndist_mat\n\n    a b c   d e\na   0 1 1   1 1\nb Inf 0 2 Inf 1\nc Inf 2 0 Inf 1\nd Inf 2 1   0 1\ne Inf 1 1 Inf 0\n\n\nHere, we can see there is at least one path of length 2 between \\(c\\) and \\(b\\). Note that Inf means that \\(i\\) cannot reach \\(j\\) through any path. To get the specific paths connecting \\(i\\) to \\(j\\) we can use the all_shortest_paths() function.\n\nigraph::all_shortest_paths(net, from = \"d\", to = \"b\", mode = \"out\")\n\n$res\n$res[[1]]\n+ 3/5 vertices, named, from 4ef57b9:\n[1] d e b\n\n\n$nrgeo\n[1] 0 1 1 1 1\n\n\n\nnrgeo is the resultant vector of values from Djikstra’s algorithm which is used to find the shortest paths.\n\nIt’s often the case that we want to summarize the distance over all \\(i,j\\) pairs. We can calculate this using the distance matrix calculated above.\n\ndiag(dist_mat) <- NA # remove the elements in the diagonal\nmean(dist_mat[dist_mat != Inf], na.rm = TRUE) # # remove Inf values\n\n[1] 1.230769\n\n\nThus, we see that nodes are (on average) separated by paths of length 1.23 (excluding pairs that cannot reach each other).\nNote that we also remove all Inf values, which means we excluded all unreachable pairs. This is a common approach but also throws out information on all cases where \\(i\\) cannot reach \\(j\\).\nCloseness\nAlternatively, we can use the “closeness” measure if we have unreachable pairs. Closeness is based on the inverse of the distance matrix. By inverting the distance matrix, all Inf values are turned into \\(0\\)s and thus can be included in the mean calculation. The inverse of the distance matrix has the opposite interpretation as above, showing show how “close” node \\(i\\) is to node \\(j\\). The disadvantage of a closeness measure is that the interpretation is not as intuitive as with distance.\n\nclose_mat <- 1 / dist_mat \nclose_mat\n\n   a   b   c  d  e\na NA 1.0 1.0  1  1\nb  0  NA 0.5  0  1\nc  0 0.5  NA  0  1\nd  0 0.5 1.0 NA  1\ne  0 1.0 1.0  0 NA\n\nmean(close_mat, na.rm = TRUE)\n\n[1] 0.575\n\n\nNote that the “mean closeness” will not mirror the “mean distance” because we have now included all unreachable pairs.\nReachability\nThis measure captures whether node \\(i\\) can reach node \\(j\\) through any path. This can be calculated directly from the distance matrix. Node \\(i\\) can reach node \\(j\\) if the distance between them is less than Inf.\n\nreach_mat <- ifelse(dist_mat == Inf, 0, 1)\nreach_mat\n\n   a  b  c  d  e\na NA  1  1  1  1\nb  0 NA  1  0  1\nc  0  1 NA  0  1\nd  0  1  1 NA  1\ne  0  1  1  0 NA\n\n\nDiameter\nWe can also use the distance matrix to calculate diameter, showing the longest geodesic (or distance) between any two nodes in the network. Diameter thus takes all of the shortest paths between nodes (i.e., distance) and calculates the longest path among that set.\n\nmax(dist_mat[dist_mat != Inf], na.rm = TRUE)\n\n[1] 2\n\nigraph::diameter(net)\n\n[1] 2\n\n\n\nLike all models, networks are just abstractions.\nMcFarland et al. (2023) suggest we view network models in terms of two theoretical perspectives and two explanatory purposes, as depicted in Table 1. This is not a definitive statement of network research, but rather a heuristic tool.\n\n\nTable 1: Networks and Research Agendas\n\n\n\n\n\n\n\nPerspective\nNetworks as Cause\nNetworks as Consequence\n\n\n\n\n\nConnectionist\n\nNetworks as pipes\n\n\nDiffusion\nPeer influence\nSocial capital\nSocial integration\nPeer selection\nSegregation\n\n\n\nPositional\n\nNetworks as roles\n\n\nPopularity effects\nRole behavior\nNetwork constraing\nExchange patterns\nNetwork stability\nCareer paths\n\n\n\n\nThis distinctions are important. For example, the average path length between nodes in a graph is correlated with information loss because “information that passes through several people is more likely to suffer distortion than information passed between only two people” (Page 2018, 119). Similarly, high betweenness scores in social networks imply that the individual will hold more information or wield more power. However, none of these interpretations make no sense when we consider networks-as-roles.\nNote. The field of network analysis has a long interdisciplinary history in sociology, anthropology, psychology, and mathematics. Due to advances in computation—and the creation of the Internet—the ability to collect network information has greatly increased and we’re starting to see the rise of a much broader (but fragmented) field of network science. Computers scientists (e.g., Kleinberg), physicists (e.g., Barabassi, Newman), and statisticians (e.g., Snijders, Hancock) have become important new players in the field. As a result, the field has begun to lack a clear integration of theories and methods.\nSEE PAGE 44 FOR FOUR THINGS\nsimilarities, relations, interactions, flows\nMemberships, in which nodes are located in the same regions in physical and social space (e.g., same neighborhoods, same department, or same club). Relations, in which nodes operate within a system of roles (e.g., father of, friend of, or teacher of) and have cognitive or affective orientations toward one another. Interactions, in which concrete interactions occur between nodes (e.g., advice, romance, or bullying). Flows, in which nodes transfer some material or cultural object, goods, information, or influence (e.g., ideas, beliefs, practices).\n\n\n\n\nMcFarland, Daniel A., Jeffrey A. Smith, James Moody, and Craig M. Rawlings. 2023. Network Analysis: Integrating Social Network Theory, Method, and Application with r. Structural Analysis in the Social Sciences. Cambridge: Cambridge University Press. https://www.cambridge.org/core/books/network-analysis/C9202FD5420BE99225FEED4B6214DBB7.\n\n\nPage, Scott E. 2018. The Model Thinker: What You Need to Know to Make Data Work for You. Basic Books."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Elster, Jon. 2015. Explaining Social Behavior: More Nuts and Bolts\nfor the Social Sciences. Cambridge University Press.\n\n\nMcFarland, Daniel A., Jeffrey A. Smith, James Moody, and Craig M.\nRawlings. 2023. Network Analysis: Integrating Social Network Theory,\nMethod, and Application with r. Structural Analysis in the Social\nSciences. Cambridge: Cambridge University Press. https://www.cambridge.org/core/books/network-analysis/C9202FD5420BE99225FEED4B6214DBB7.\n\n\nPage, Scott E. 2018. The Model Thinker: What You Need to Know to\nMake Data Work for You. Basic Books."
  },
  {
    "objectID": "net-02-blockmodels.html#structural-positions",
    "href": "net-02-blockmodels.html#structural-positions",
    "title": "4  Blockmodels",
    "section": "4.1 Structural Positions",
    "text": "4.1 Structural Positions\nFinding similar positions in a network means that we identify collections of individuals that are similarly embedded in networks of relations.\n\nWhere two nodes have similar sets of ties to all other nodes (though they may not be themselves connected), we speak of structural equivalence (Lorrain and White 1971). A pair is perfectly structurally equivalent if each actor has precisely identical ties and nonties to all third parties. For example, donors who give funds to the same party candidates jointly occupy a position yet do not exchange money among themselves. Hence, the structurally equivalent members of a dyad are redundant and indistinguishable from one another. In other words, both entities map directly onto one another in a graph and could be swapped without any change in the network’s structure.\nKnoke et al. (2021, 45)\n\nIn other words, two nodes are structurally equivalent when they have identical ties to every other node in the network. There are different types of equivalence. For example, two nodes are considered to be automorphically equivalent if their positions share all relevant graph-theoretic properties (e.g., degree, centrality, reachability, and so on). This differs from structural equivalence because there’s no requirement that nodes be connected with the exact set of other nodes. And the notion of regular equivalence applies to two nodes that simply have similar profiles of ties to other nodes.\nNote. There’s a resemblance between the idea of structural equivalence in social network analysis and the idea of synonymy in word embeddings. In both approaches, similarity is not based on co-occurrences but rather on shared co-occurrences.\n\n4.1.1 Blockmodeling\nBlockmodel analysis begins by computing measures of similarity for every pair of rows and columns in a matrix. The first attempt at blockmodeling used Pearson correlation coefficients as a measure of similarity and a computer algorithm called CONCOR (White, Boorman, and Breiger 1976; Boorman and White 1976).\n\nThe algorithm produces successive bifurcations into 2-block, 4-block, 8-block, and so forth solutions; the analyst decides where to stop the splitting process. Next, the rows and columns are rearranged (permuted) so that entities belonging to each block are adjacent to one another within submatrices (i.e., structural equivalence classes). Then the densities of ties within and between blocks are calculated (ignoring self-ties), yielding a block density matrix. Finally, a simplified binary image matrix is constructed by replacing every block density greater than the overall network density with a 1 and any block with below-mean density by a 0. Subsequently, alternative measures of similarity and structural equivalence were deployed for blockmodeling, including Euclidean distance, city-block (Manhattan) distance, Hamming distance, Jaccard index, and simple matching coefficients.\nKnoke et al. (2021, 46)\n\nThe goal is to identify clusters of nodes that share structural characteristics. This technique gets its name from the reordering (or permuting) of rows and columns in an adjacency matrix into “blocks,” the members of which are based on positions occupied by the nodes in network structure. As with community detection, the goal is to identify subgroups in a network and their interrelations; however, the results may be different. Some blocks may end up being communities, but this is not necessarily the case, nor is it the goal of blockmodeling to do this.\n\nThe actors within a cluster should have the same (or similar) pattern of ties, and actors in different clusters should be also connected through specific patterns of ties. If we present a relation by using a matrix, we can rearrange the matrix in such a way that the actors belonging to the first cluster are given first (by rows and by columns), then the actors of the second cluster, and so on. The relational matrix can then be partitioned by the clusters into several blocks.\nDoreian, Batagelj, and Ferligoj (2004, 11–12)\n\n\nADD STUFF FROM KNOKE\nADD CHAPTER ON TWO-MODE BLOCKMODELING IN DOREIAN\nhttps://acastroaraujo-notebooks.netlify.app/posts/2022-11-05-multimodal-networks/index.html#structural-positions"
  },
  {
    "objectID": "net-02-blockmodels.html#communities",
    "href": "net-02-blockmodels.html#communities",
    "title": "4  Blockmodels",
    "section": "4.2 Communities",
    "text": "4.2 Communities\n\n\n\n\nBoorman, Scott A., and Harrison C. White. 1976. “Social Structure from Multiple Networks. II. Role Structures.” American Journal of Sociology 81 (6): 13841446.\n\n\nDoreian, Patrick, Vladimir Batagelj, and Anuška Ferligoj. 2004. “Generalized Blockmodeling of Two-Mode Network Data.” Social Networks 26 (1): 2953.\n\n\nKnoke, David, Mario Diani, James Hollway, and Dimitris Christopoulos. 2021. Multimodal Political Networks. Cambridge University Press.\n\n\nWhite, Harrison C., Scott A. Boorman, and Ronald L. Breiger. 1976. “Social Structure from Multiple Networks. I. Blockmodels of Roles and Positions.” American Journal of Sociology 81 (4): 730780."
  },
  {
    "objectID": "net-03-ergm.html",
    "href": "net-03-ergm.html",
    "title": "5  ERGMs",
    "section": "",
    "text": "Exponential Random Graph Models (ERGM).\nTake a network \\(G\\) in adjacency matrix form \\(\\mathbf X\\). Let \\(\\mathcal N\\) represent the set of possible permutations of the network \\(\\mathbf X\\), including the completely empty network (no edges) and the completely connected network (every node is connected to every other node).\n\\[\n\\Pr(\\mathbf X, \\boldsymbol \\theta) = \\frac{\\exp(\\boldsymbol \\theta^\\top \\mathbf z(\\mathbf X))}{\\kappa (\\boldsymbol \\theta)}\n\\]\nHere, \\(\\boldsymbol \\theta\\) is a vector of parameters to be estimated, \\(\\mathbf z(\\mathbf X)\\) is a vector of network statistics (e.g., number of edges, number of triangles), and \\(\\kappa (\\boldsymbol \\theta)\\) is just a normalizing constant.\n\n\\(\\kappa (\\mathbf X) = \\sum_{\\mathbf{X}^* \\in \\mathcal N} \\exp(\\boldsymbol \\theta^\\top \\mathbf z(\\mathbf X))\\)\nThis is usually unfeasible to compute, which is why we end up using approximation methods.\n\nA network statistic, for example, can capture something like reciprocity (i.e., the tendency of connections to be mutual). Here we just count the total number of reciprocal ties:\n\\[\n\\mathbf z_r(\\mathbf X) = \\sum_{i < j} \\mathbf{X}_{ij} \\cdot \\mathbf{X}_{ji}\n\\]\nBut we can add much, much more.\n\nLogistic Regression:\n\\[\n\\Pr(X_{ij} = 1) = \\text{logit}^{-1} \\big(\\alpha_i + \\alpha_j +  \\big)\n\\]\nDegeneracy\nhttps://statnet.org/workshop-ergm/ergm_tutorial.html"
  },
  {
    "objectID": "net-03-ergm.html#introduction",
    "href": "net-03-ergm.html#introduction",
    "title": "5  ERGMs",
    "section": "5.1 Introduction",
    "text": "5.1 Introduction\n\nThe purpose of ERGMs, in a nutshell, is to describe parsimoniously the local selection forces that shape the global structure of a network.\nHunter et al. (2008, 2)\n\nExponential Random Graphs Models attempt to predict the ties in a network—i.e., it describes tie-formation processes. An ERGM takes the entire observed network as one instance of a probability distribution for all possible networks with the same number of nodes, including the completely empty network (no ties) and the completely connected network (every node is connected to every other node).\n\nERGMs are also known as p-star models.\n\nNotation.\nA network is represented as an adjacency matrix \\(\\mathbf Y\\) which is also a random variable; \\(y\\) is a specific realization of that variable—i.e., the network we actually get to observe.\n\\[\ny_{ij} = \\begin{cases}\n1 & \\text{if } i \\text{ sends a tie to } j \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\n\n\\(y_{ij} = y_{ji}\\) when \\(\\mathbf Y\\) is undirected\n\nThe ERGM specifies the probability of a set of ties \\(\\Pr(\\mathbf Y = y)\\) given a set of persons and their attributes. More importantly, we posit that this probability depends on the number of certain configurations of ties which reflect particular social processes (e.g., reciprocity, transitivity, homophily). Some of these processes assume dyadic independence (e.g., homophily) and some assume dyadic dependence, which the likelihood of a dyad \\(y_{ij}\\) is dependent on the existence of other dyads; for example, transitivity implies that if \\(y_{ij}\\) and \\(y_{jk}\\) both exist, then \\(y_{ki}\\) is more likely to exist. These processes are represented with a vector \\(\\boldsymbol z\\) of network statistics.\n\nTie-formation mechanisms are context-dependent—e.g., friendship networks are marked by reciprocity and homophily; patronage networks are expected to be asymmetric, non-transitive, and heterophilous; scientific citations are marked by preferential attachment; etc.\n\nFor example, the number of reciprocal ties (reciprocity) can be quantified as follows:\n\\[\n\\boldsymbol z_r(\\mathbf Y) = \\sum_{i < j} \\mathbf{Y}_{ij} \\times \\mathbf{Y}_{ji}\n\\]\n\nreciprocity example\n\nSee Morris, Handcock, and Hunter (2008) for a list of widely used network statistics and other covariates implemented in the ergm package.\nERGMs\nThe probability distribution of a random graph can be parameterized as follows:\n\\[\n\\begin{align}\n\\Pr(\\mathbf Y = y)  &= \\frac{1}{\\kappa (\\boldsymbol \\theta)} \\exp \\bigg(\\sum_{k=1}^K \\theta_k z_k(y)\\bigg) \\\\\\\\\n&= \\frac{1}{\\kappa (\\boldsymbol \\theta)} \\exp \\Big(\n\\boldsymbol \\theta^\\top \\boldsymbol z(y) \\Big)\n\\end{align}\n\\tag{5.1}\\]\n\nThe normalizing constant can be expressed as follows:\n\\[\n\\kappa(\\boldsymbol \\theta) = \\sum_{y \\in \\mathcal N} \\exp \\Big(\\boldsymbol \\theta^\\top \\boldsymbol z(y) \\Big),\n\\]\nwhere \\(\\mathcal N\\) is the support of \\(\\mathbf Y\\)—i.e., the set of all potential networks realizable by \\(\\mathbf Y\\).\n\nThe \\(\\boldsymbol{z}(y)\\) vector represents covariates and the \\(\\boldsymbol \\theta\\) vector represents coefficients to be estimated. This vector can be expanded by replacing \\(\\boldsymbol{z}(y)\\) with \\(\\boldsymbol{z}(y, \\mathbf{X})\\) to allow for additional covariate information about the network.\nThe denominator \\(\\kappa\\) represents the quantity in the numerator summed over all possible networks of the same size. Specifing this number is an important yet often overlooked aspect of an ERGM. Furthermore, the set of all possible networks \\(\\mathcal N\\) tends to be very big and so it is usually unfeasible to compute \\(\\kappa\\) directly; hence, these models are usually fit using some approximation method (like MCMC) instead of more traditional maximum likelihood estimation.\nOnce we have a vector \\(\\boldsymbol \\theta\\) of parameters, ERGMs can be used to generate new networks. Thus, we can then study the types of networks produced, on average, by the tie-formation processes incorporated in an ERGM.\nChange statistics\nAn alternative specification of Equation 5.1 may clarify the interpretation of the coefficients in \\(\\boldsymbol \\theta\\). A vector of change statistics \\(\\boldsymbol \\delta\\)is a function of three things: (1) a particular choice of \\(\\boldsymbol z\\) statistics defined on a network; (2) a particular network \\(y\\); and (3) a particular collection of dyad indices \\((i, j)\\). With this in place, we define a vector of change statistics as follows:\n\\[\n\\boldsymbol{\\delta_{z}}(y)_{ij} = \\boldsymbol{z}(y_{ij}^+) - \\boldsymbol{z}(y_{ij}^-)\n\\]\nHere, \\(y_{ij}^+\\) and \\(y_{ij}^-\\) represent the networks realized respectively by fixing \\(y_{ij} = 1\\) and \\(y_{ij} = 0\\) while keeping the rest of the network exactly as in \\(y\\). Thus, \\(\\boldsymbol{\\delta_z}(y)_{ij}\\) is the change in the value of the network statistic vector \\(\\boldsymbol z\\) that would occur if \\(y_{ij}\\) where changed from \\(0\\) to \\(1\\).\nERGMs as logistic regression\nEquation 5.1 can be expressed, with the help of the change statistic vector, as a logistic regression for dyads:\n\\[\n\\begin{align}\n\\Pr(Y_{ij} = 1 \\mid \\mathbf{Y}_{ij}^c = y_{ij}^c) &= \\text{logit}^{-1} \\Bigg(\n\\sum_{k = 1}^K \\theta_k \\delta_{\\boldsymbol{z}_k}(y)_{ij} \\Bigg) \\\\\\\\ &=\n\\text{logit}^{-1} \\Big( \\boldsymbol \\theta^\\top \\boldsymbol{\\delta_z}(y)_{ij}\n\\Big)\n\\end{align}\n\\tag{5.2}\\]\nHere, \\(Y_{ij}^c\\) denotes all dyads other than \\(Y_{ij}\\) and \\(\\delta z_k (y)\\) is the amount by which \\(z_k(y)\\) changes when \\(Y_{ij}\\) is toggled from 0 to 1. The presence of \\(Y_{ij}^c\\) in the conditional reflects the mutual dependence of ties. This clarifies the interpretation of \\(\\boldsymbol \\theta\\)—i.e., if forming a tie increases \\(z_k\\) by 1 then (all else equal) the log-odds of that tie forming increase by \\(\\theta_k\\). Note that a single tie may affect multiple \\(z\\) statistics."
  },
  {
    "objectID": "net-03-ergm.html#assumptions",
    "href": "net-03-ergm.html#assumptions",
    "title": "5  ERGMs",
    "section": "5.2 Assumptions",
    "text": "5.2 Assumptions\n\nThe homogeneity assumption. In its most general form, this model could include \\(d – 1\\) terms, where \\(d\\) is the number of dyads in the network. That would be a saturated model with every tie having its own probability. As in any statistical model, the goal is to find a parsimonious representation, and proposing homogeneous classes of dyads is a common form of parsimony. Any \\(z\\) statistic counting the aggregate number of some configuration (e.g., triangles) makes an implicit homogeneity assumption, that is, that all counted instances are equiprobable. This is similar to the assumption in linear regression that a covariate’s effect is the same for all observations…\nDyadic dependence and independence. When a model includes only terms that represent the composition of node attributes within ties, it is similar to traditional logistic or log-linear models for contingency tables (Koehly, Goodreau, and Morris 2004). Such models are said to exhibit dyadic independence because the probability of any tie does not depend on the value of other ties, only on the attributes of the two actors involved. As a result, \\(Y^c_{ij}\\) can be removed from the conditional in Equation 5.2, yielding a set of independent observations amenable to standard statistical analysis.\n[…]\nEstimation and model degeneracy.\nGiven a proposed model (set of \\(z\\) statistics), one would like to identify the \\(\\boldsymbol \\theta\\) vector maximizing model likelihood. However, the normalizing constant \\(c\\) in Equation 5.1 is impossible to calculate for all but the smallest networks, preventing direct evaluation of the likelihood function…\nThe likelihood can be approximated using Markov chain Monte Carlo simulation methods (MCMC; Geyer and Thompson 1992; Snijders 2002), which generate a sample from the space of possible networks to estimate the \\(\\boldsymbol \\theta\\) vector. This approach has additional benefits: the same algorithm may be used to simulate network realizations for a given \\(\\boldsymbol \\theta\\) vector with appropriate probability. A sample of such realizations provides a means for examining model fit (Hunter, Goodreau, and Handcock 2008).\nThis behavior, termed model degeneracy, was explored in detail by Handcock (2003a, 2003b). The intuition behind degeneracy is relatively straightforward. If we specify a model that is unlikely to produce the observed network, then one of two things can happen when it is fit to data: either the MLEs do not exist and estimation does not converge (as in the extremal case above), or the MLEs exist but do not provide a good fit to the data (as in the bimodal case above). Degeneracy is not a shortcoming of the MCMC estimation procedure; nor does it necessarily imply that our intuition about the general process (e.g., that triangles tend to become closed) is wrong. It just implies that the process is not properly specified (e.g., that closure occurs homogeneously across all actor pairs). The solution is to specify a better-fitting model for the data, but this is less straightforward for networks than for other statistical contexts. In ERG modeling, a misspecified model can fail to converge, yielding no parameter estimates to guide model diagnosis or respecification.\nMany of the potential solutions to degeneracy replace strong homogeneity assumptions with some form of heterogeneity that defines the scope or form of dependence. One plausible feature of the triad closure case may be a differential tendency for triad closure by attribute combinations—for example, that only triads comprising three actors with shared attributes tend to close. Another mechanism entails a decreasing marginal impact in the effect of triangles on tie formation. To capture this phenomenon, Hunter (2007) and Hunter and Handcock (2006) described a statistic called the geometrically weighted edgewise shared partner distribution (GWESP), a reparameterization of a statistic of Snijders et al. (2006). The shared partner distribution is an alternative approach to counting triangles. Two actors “share” a partner if both have a tie to the same partner, and each shared partner forms a triangle if the original pair are tied. In contrast to the census of triangles or the clustering coefficient (which produce a single measure for the whole network), the shared partner count is taken on each edge (hence the “edgewise”), producing a distribution of counts. The GWESP statistic defines a parametric form of this count distribution that gives each additional shared partner a declining positive impact on the probability of two persons forming a tie. This approach has a clear interpretation and has been shown to work well in practice in overcoming model degeneracy and producing models that fit a wide range of data well…\nGoodreau, Kitts, and Morris (2009, 109–11)\n\nMore references: Hunter et al. (2008); McFarland et al. (2014); Wimmer and Lewis (2010); Krivitsky and Morris (2017); Smith (2012); Goodreau, Kitts, and Morris (2009); Robins and Morris (2007); Robins et al. (2007); Krivitsky, Handcock, and Hunter (2019)\nWhy can’t we just turn our adjacency matrix into long-form so that the unit of observation is a dyad and then predict the presence of ties using logistic regression?\n\n…the number of observations in such dyadic datasets are much larger than the number of individuals, because each individual appears (nxn)-n times as a possible outcome of every other individual’s choice set and as the possible chooser of every other individual. Consequently, standard errors will be biased downward as the sample size is artificially inflated, and correlated errors are introduced through repeated observations of individuals as both senders and receivers of ties."
  },
  {
    "objectID": "net-03-ergm.html#examples",
    "href": "net-03-ergm.html#examples",
    "title": "5  ERGMs",
    "section": "5.3 Examples",
    "text": "5.3 Examples\n\n\nCode\nlibrary(ergm)\n\n\nhttps://statnet.org/workshop-ergm/ergm_tutorial.html\nhttps://bookdown.org/markhoff/social_network_analysis/homophily-and-exponential-random-graphs-ergm.html\n\n\n\n\nGoodreau, Steven M., James A. Kitts, and Martina Morris. 2009. “Birds of a Feather, or Friend of a Friend? Using Exponential Random Graph Models to Investigate Adolescent Social Networks.” Demography 46: 103125.\n\n\nHunter, David R., Mark S. Handcock, Carter T. Butts, Steven M. Goodreau, and Martina Morris. 2008. “Ergm: A Package to Fit, Simulate and Diagnose Exponential-Family Models for Networks.” Journal of Statistical Software 24 (3): nihpa54860.\n\n\nKrivitsky, Pavel N., Mark S. Handcock, and David R. Hunter. 2019. Ergm.count: Fit, Simulate and Diagnose Exponential-Family Models for Networks with Count Edges. https://CRAN.R-project.org/package=ergm.count.\n\n\nKrivitsky, Pavel N., and Martina Morris. 2017. “Inference for Social Network Models from Egocentrically Sampled Data, with Application to Understanding Persistent Racial Disparities in HIV Prevalence in the US.” The Annals of Applied Statistics 11 (1): 427.\n\n\nMcFarland, Daniel A., James Moody, David Diehl, Jeffrey A. Smith, and Reuben J. Thomas. 2014. “Network Ecology and Adolescent Social Structure.” American Sociological Review 79 (6): 1088–1121. https://doi.org/10.1177/0003122414554001.\n\n\nMorris, Martina, Mark S. Handcock, and David R. Hunter. 2008. “Specification of Exponential-Family Random Graph Models: Terms and Computational Aspects.” Journal of Statistical Software 24 (4): 1548.\n\n\nRobins, Garry, and Martina Morris. 2007. “Advances in Exponential Random Graph (p*) Models.” Social Networks, Special Section: Advances in Exponential Random Graph (p*) Models, 29 (2): 169–72. https://doi.org/10.1016/j.socnet.2006.08.004.\n\n\nRobins, Garry, Pip Pattison, Yuval Kalish, and Dean Lusher. 2007. “An Introduction to Exponential Random Graph (p*) Models for Social Networks.” Social Networks, Special Section: Advances in Exponential Random Graph (p*) Models, 29 (2): 173–91. https://doi.org/10.1016/j.socnet.2006.08.002.\n\n\nSmith, Jeffrey A. 2012. “Macrostructure from Microstructure: Generating Whole Systems from Ego Networks.” Sociological Methodology 42 (1): 155205.\n\n\nWimmer, Andreas, and Kevin Lewis. 2010. “Beyond and Below Racial Homophily: ERG Models of a Friendship Network Documented on Facebook.” American Journal of Sociology 116 (2): 583–642. https://doi.org/10.1086/653658."
  },
  {
    "objectID": "net-03-ergm.html#exponential-random-graph-models",
    "href": "net-03-ergm.html#exponential-random-graph-models",
    "title": "5  ERGMs",
    "section": "5.1 Exponential Random Graph Models",
    "text": "5.1 Exponential Random Graph Models\n\nThe purpose of ERGMs, in a nutshell, is to describe parsimoniously the local selection forces that shape the global structure of a network.\nHunter et al. (2008, 2)\n\nExponential Random Graphs Models attempt to predict the ties in a network—i.e., it describes tie-formation processes. An ERGM takes the entire observed network as one instance of a probability distribution for all possible networks with the same number of nodes, including the completely empty network (no ties) and the completely connected network (every node is connected to every other node).\n\nERGMs are also known as p-star models.\n\nNotation.\nA network is represented as an adjacency matrix \\(\\mathbf Y\\) which is also a random variable; \\(y\\) is a specific realization of that variable—i.e., the network we actually get to observe.\n\\[\ny_{ij} = \\begin{cases}\n1 & \\text{if } i \\text{ sends a tie to } j \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\n\n\\(y_{ij} = y_{ji}\\) when \\(\\mathbf Y\\) is undirected\n\nThe ERGM specifies the probability of a set of ties \\(\\Pr(\\mathbf Y = y)\\) given a set of persons and their attributes. More importantly, we posit that this probability depends on the number of certain configurations of ties which reflect particular social processes (e.g., reciprocity, transitivity, homophily). Some of these processes assume dyadic independence (e.g., homophily) and some assume dyadic dependence, which the likelihood of a dyad \\(y_{ij}\\) is dependent on the existence of other dyads; for example, transitivity implies that if \\(y_{ij}\\) and \\(y_{jk}\\) both exist, then \\(y_{ki}\\) is more likely to exist. These processes are represented with a vector \\(\\boldsymbol z\\) of network statistics.\n\nTie-formation mechanisms are context-dependent—e.g., friendship networks are marked by reciprocity and homophily; patronage networks are expected to be asymmetric, non-transitive, and heterophilous; scientific citations are marked by preferential attachment; etc.\n\nFor example, the number of reciprocal ties (reciprocity) can be quantified as follows:\n\\[\n\\boldsymbol z_r(\\mathbf Y) = \\sum_{i < j} \\mathbf{Y}_{ij} \\times \\mathbf{Y}_{ji}\n\\]\n\nreciprocity example\n\nSee Morris, Handcock, and Hunter (2008) for a list of widely used network statistics and other covariates implemented in the ergm package.\nERGMs\nThe probability distribution of a random graph can be parameterized as follows:\n\\[\n\\begin{align}\n\\Pr(\\mathbf Y = y)  &= \\frac{1}{\\kappa (\\boldsymbol \\theta)} \\exp \\bigg(\\sum_{k=1}^K \\theta_k z_k(y)\\bigg) \\\\\\\\\n&= \\frac{1}{\\kappa (\\boldsymbol \\theta)} \\exp \\Big(\n\\boldsymbol \\theta^\\top \\boldsymbol z(y) \\Big)\n\\end{align}\n\\tag{5.1}\\]\n\nThe normalizing constant can be expressed as follows:\n\\[\n\\kappa(\\boldsymbol \\theta) = \\sum_{y \\in \\mathcal N} \\exp \\Big(\\boldsymbol \\theta^\\top \\boldsymbol z(y) \\Big),\n\\]\nwhere \\(\\mathcal N\\) is the support of \\(\\mathbf Y\\)—i.e., the set of all potential networks realizable by \\(\\mathbf Y\\).\n\nThe \\(\\boldsymbol{z}(y)\\) vector represents covariates and the \\(\\boldsymbol \\theta\\) vector represents coefficients to be estimated. This vector can be expanded by replacing \\(\\boldsymbol{z}(y)\\) with \\(\\boldsymbol{z}(y, \\mathbf{X})\\) to allow for additional covariate information about the network.\nThe denominator \\(\\kappa\\) represents the quantity in the numerator summed over all possible networks of the same size. Specifing this number is an important yet often overlooked aspect of an ERGM. Furthermore, the set of all possible networks \\(\\mathcal N\\) tends to be very big and so it is usually unfeasible to compute \\(\\kappa\\) directly; hence, these models are usually fit using some approximation method (like MCMC) instead of more traditional maximum likelihood estimation.\nOnce we have a vector \\(\\boldsymbol \\theta\\) of parameters, ERGMs can be used to generate new networks. Thus, we can then study the types of networks produced, on average, by the tie-formation processes incorporated in an ERGM.\nChange statistics\nAn alternative specification of Equation 5.1 may clarify the interpretation of the coefficients in \\(\\boldsymbol \\theta\\). A vector of change statistics \\(\\boldsymbol \\delta\\)is a function of three things: (1) a particular choice of \\(\\boldsymbol z\\) statistics defined on a network; (2) a particular network \\(y\\); and (3) a particular collection of dyad indices \\((i, j)\\). With this in place, we define a vector of change statistics as follows:\n\\[\n\\boldsymbol{\\delta_{z}}(y)_{ij} = \\boldsymbol{z}(y_{ij}^+) - \\boldsymbol{z}(y_{ij}^-)\n\\]\nHere, \\(y_{ij}^+\\) and \\(y_{ij}^-\\) represent the networks realized respectively by fixing \\(y_{ij} = 1\\) and \\(y_{ij} = 0\\) while keeping the rest of the network exactly as in \\(y\\). Thus, \\(\\boldsymbol{\\delta_z}(y)_{ij}\\) is the change in the value of the network statistic vector \\(\\boldsymbol z\\) that would occur if \\(y_{ij}\\) where changed from \\(0\\) to \\(1\\).\nERGMs as logistic regression\nEquation 5.1 can be expressed, with the help of the change statistic vector, as a logistic regression for dyads:\n\\[\n\\begin{align}\n\\Pr(Y_{ij} = 1 \\mid \\mathbf{Y}_{ij}^c = y_{ij}^c) &= \\text{logit}^{-1} \\Bigg(\n\\sum_{k = 1}^K \\theta_k \\delta_{\\boldsymbol{z}_k}(y)_{ij} \\Bigg) \\\\\\\\ &=\n\\text{logit}^{-1} \\Big( \\boldsymbol \\theta^\\top \\boldsymbol{\\delta_z}(y)_{ij}\n\\Big)\n\\end{align}\n\\tag{5.2}\\]\nHere, \\(Y_{ij}^c\\) denotes all dyads other than \\(Y_{ij}\\) and \\(\\delta z_k (y)\\) is the amount by which \\(z_k(y)\\) changes when \\(Y_{ij}\\) is toggled from 0 to 1. The presence of \\(Y_{ij}^c\\) in the conditional reflects the mutual dependence of ties. This clarifies the interpretation of \\(\\boldsymbol \\theta\\)—i.e., if forming a tie increases \\(z_k\\) by 1 then (all else equal) the log-odds of that tie forming increase by \\(\\theta_k\\). Note that a single tie may affect multiple \\(z\\) statistics."
  }
]