# Introduction

@page2018 offers a collection of models that span multiple disciplines. All these models share a common form: they assume a set of entities (e.g. people or organizations) *and* they describe how they interact.

They share three features:

-   *They simplify ---*i.e., "stripping away unnecessary details, abstracting from reality".

-   *They formalize*, they use precise definitions in mathematical form ---e.g., a model might represent beliefs as probability distributions over states of the world.

-   *They are wrong*, this is true of all models ---i.e., they simplify, they omit details (sometimes crucial ones).

There are also three ways of thinking about what models are:

1.  *Simplifications of the world* ---i.e. the embodiment approach.

    These models include the important parts of a problem, while stripping away (or lumping together) the less salient parts of a problem.

    This approach takes a **realist view of models**.

2.  *Mathematical analogies*.

    This approach simplifies a problem by making unrealistic assumptions.

    The spherical cow: to estimate the amount of leather in a cowhide, we *assume* a spherical cow. We do this for mathematical convenience: we know how to integrate over $\textsf{tan}(x)$ and $\textsf{cos}(x)$, but not over $\textsf{cow}(x)$.

    We usually judge these models by their *predictive accuracy*.

3.  *The alternative reality approach*.

    These models function as analytic and computational playgrounds in which we can explore possibilities. This is, for example, what we do with *agent-based models*.

    In this scenario, computer simulations become an important tool.

Unlike the first approach, these two approaches take an **instrumental view of models***.*

## Uses

\[R\] **Reason**. *To identify conditions and deduce logical implications.*

<aside>...or **`REDCAPE`**</aside>

While it's true that the conclusions we derive depend on our assumptions, this doesn't mean *all* models deliver **tautologies**. The logical implications of models are sometimes unexpected ---e.g. *Arrow's impossibility theorem*. Other times they will solve paradoxes ---e.g. using causal models to solve *Simpson's paradox*. And sometimes they will simply uncover mathematical relationships ---e.g. prices and marginal costs.

Above all, *logic reveals the conditionality of truths*; they identify the conditions under which certain claims hold or don't.

> Critics of formalism claim that models repackage what we already know, that they pour old wine into shiny mathematical bottles, that we do not need a model to know that two heads are better than one or that he who hesitates is lost. We can learn the value of commitment from reading of Odysseus tying himself to the mast. That criticism fails to recognize that inferences drawn from models take conditional forms: if $A$ holds, then $B$ follows. Lessons drawn from literature or proverbial advice from great thinkers often provide no conditions. If we try to lead our lives or manage others by unconditional truths, we find ourselves lost in a sea of *opposite proverbs*.
>
> @page2018 [pp. 18]

How can we choose between the following list without knowing which conditions make them true?

| Proverb                              | Opposite                           |
|-------------------------------------|-----------------------------------|
| Two heads are better than one        | Too many cooks spoil the broth     |
| He who hesitates is lost             | A stitch in time saves nine        |
| Tie yourself to the mast             | Keep your options open             |
| The perfect is the enemy of the good | Do it well or not at all           |
| Actions speak louder than words      | The pen is mightier than the sword |

\[E\] **Explain**. *To provide (testable) explanations for empirical phenomena.*

Note that models can also explain *shape*: e.g. network models of influence and contagion often produce S-shape curves.

> As for the claim that models can explain anything: it is true, they can. However, a model-based explanation includes formal assumptions and explicit causal chains. Those assumptions and causal chains can be taken to data. A model that claims that high levels of criminal behavior can be explained by low probabilities of being caught can be tested.
>
> @page2018 [pp. 19]

\[D\] **Design**. *To choose features of institutions, policies, and rules.*

\[C\] **Communicate**. Models improve communication by creating a common representation, thus enabling the transferring of ideas between different communities of inquiry.

\[A\] **Act**. *To guide policy choices and strategic actions.*

\[P\] **Predict**. *To make numerical and categorical predictions of future and unknown phenomena*. Note that prediction differs from explanation.

For example, deep-learning algorithms can predict many things with great accuracy, but they offer little in the way of explanation. On the other hand, some models can explain but have little predictive power. This is related to Elster's [-@elster2015] discussion of the fact that we don't know which "social mechanism" will be triggered in any given situation.

\[E\] **Explore**. *To investigate possibilities and hypotheticals* ---i.e., the "alternative reality approach".

## Human Behavior

@page2018 notes that modeling human behavior is challenging because people share six characteristics: we are *diverse;* we are *socially influenced;* we are *error-prone;* we are *purposive;* we *learn;* and we have *agency.*

> Each of this six characteristics are potential model features. If we include a feature, we must decide how much of it to include. How diverse do we make our actors? How much social influence do we include? Do people learn from others? How do we define objectives? How much agency do people possess?
>
> @page2018 [pp. 47]

For example, to tackle diversity we sometimes assume that behavioral diversity cancels out. And this will only happen if the actions people take are *independent* (i.e. models of normal distributions).

That being said, @page2018 divides his repertoire of models according to whether we believe humans to follow a logic of consequence (i.e., rational decision-makers) or they follow a logic of appropriateness (i.e., rule-followers).

### Rational-choice models

An agent is rational if she makes choices towards *fulfilling a goal*, making the most *efficient* use of resources.

> An individual's preferences are represented by a mathematical utility or payoff function defined over a set of possible actions. The individual chooses the action that maximizes the function's value. In a *game*, that choice may require beliefs about the actions of other players.
>
> @page2018 [pp. 48]

In assuming a utility function, we give preferences a coherency that may not exist. These preferences must satisfy certain axioms in order to be representable by a utility function: completeness, transitivity, independence, and continuity.

Suppose we have a choice set $X = \{x_1, x_2, \dots, x_n\}$ that contains all available alternatives (e.g. stuff to buy, decisions to make).

-   *Completeness:* all pairs of alternatives can be compared.

-   *Transitivity:* a logical order can be established among them: if $x_1 \succeq x_2$ and $x_2 \succeq x_3$, then $x_1 \succeq x_3$.

-   *Independence of irrelevant alternatives.*

-   *Continuity:* if we have $x_1 \succeq x_2 \succeq x_3$, then there exists a probability $p$ such that $x_2 = p\ x_1 + (1-p) x_3$.

People will violate this axioms under any number of circumstances, leading to a widespread skepticism of rational-actor models. Page responds to these criticisms with four arguments, depicted in @tbl-args-rat-choice.

| Argument         | Description                                                                                                 |
|:--------------|---------------------------------------------------------|
| **"As if"**      | Intelligent rule-based behavior may be indistinguishable from optimal or near-optimal behavior              |
| **Learning**     | In situations that are repeated, people should approach optimal behavior                                    |
| **Large stakes** | On important decisions, people gather information and think slowly                                          |
| **Uniqueness**   | Optimal behavior is often unique, making the model testable/ tractable                                      |
| **Consistency**  | Optimal behavior creates a consistent model. If people learn the model, they will not change their behavior |
| **Benchmark**    | Optimal behavior provides a benchmark as an *upper bound* on people's cognitive abilities                   |

: **Arguments for Rational Choice** {#tbl-args-rat-choice}

The consistency argument is related to "Lucas' critique", discussed near the end. Basically, any model that doesn't predict optimal behavior will fail to make long-term predictions when people have something to gain by optimizing their behavior.

Also, numerous studies on "heuristics and cognitive bias" (e.g. *loss aversion* and *hyperbolic discounting*) have shown systematic deviations from rational choice.

These considerations aside, Page argues that we should always be open to the possibility that rational-actor models will not solve the problem at hand, and that we should privilege other models instead.

### Rule-based models

Whereas optimization-based models assume an underlying utility or payoff function that people maximize, rule-based models assume specific behaviors. Many people equate optimization-based models with mathematics and rule-based models with computation, but this distinction is not very clean.

-   **Fixed rules**. A fixed rule applies the same algorithm (or decision making protocol) at all times. This will provide a *lower bound* on people's cognitive abilities. For example, "zero intelligence" is sometimes used as a fixed rule in markets: zero-intelligence traders accept *any* offer that produces a payoff. Remarkably, encoding this rule in a computer model results in nearly efficient outcomes.

-   **Adaptive rules**. An adaptive rule switches among a set of behaviors, evolves new behaviors, or copies the behaviors of others in order to improve a payoff. Thus, adaptive rules require a utility or payoff function. People like Gerd Gigerenzer argue that people tend toward simple and effective rules within any given situation, and that if that's what people do, then we should model them this way.

Note that rule-based models make no explicit assumption about rationality, but adaptive-rule models exhibit "ecological rationality"---i.e., better rules eventually predominate.

Finally, note that some rule-based behaviors approximate rational choice. For example, "buy low, sell high" is a very simple heuristic that will consistently lead to profit in a market economy. Rules like these might be held consciously or unconsciously.

**How smart should we make the actors in our models?**

It depends on what type of outcome is produced by the model. We have four options: *equilibrium*, *cycles*, *randomness*, or *complexity*. If the model produces randomness at an aggregate level, then it's safe to say that individuals probably can't learn anything ---i.e. they *can't* choose optimally. The models that produce cycles or equilibria, on the other hand, create a *stationary environment* in which we expect people to learn.

Notice that if adaptive rules produce an equilibrium, then the equilibrium should be consistent with behavior by optimizing agents. Otherwise, *optimal behavior will be an unrealistic assumption in complex situations.*

**The Lucas critique**

If people learn, then we cannot rely on past data to predict outcomes under a policy change. This insight is a variant of *Campbell's law*, which states that people respond to any measure in ways that render it less effective. Thus, models must take into account the fact that people respond to policy and environmental changes.
